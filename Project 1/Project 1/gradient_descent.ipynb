{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the same results with train and train_manual_update\n",
    "- Write torch.manual_seed(42) at the beginning of your notebook.\n",
    "- Write torch.set_default_dtype(torch.double) at the beginning of your notebook to alleviate precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "# Set device for the training\n",
    "DEVICE_ = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Using device: {DEVICE_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3\n",
    "datasets: training, validation and test. Take a subset of these datasets\n",
    "by keeping only 2 labels: cat and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \"\"\"\n",
    "    Rezize to 16x16 and normalize\n",
    "    Load CIFAR-10 dataset and then filter for cars and cats\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize((16, 16)),  # Resize to get 768 input features\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    \n",
    "    # Load full training and test datasets\n",
    "    train_full = datasets.CIFAR10(data_path, train=True, download=True, transform=preprocessor)\n",
    "    test_full = datasets.CIFAR10(data_path, train=False, download=True, transform=preprocessor)\n",
    "    # Printing classes for finding cars and cats\n",
    "    print(\"Classes in CIFAR-10 dataset:\", train_full.classes)\n",
    "    \n",
    "    # Filter for cats (class 3) and cars(Automobile) (class 1) \n",
    "    def filter_ds(dataset, classes=[1, 3]):\n",
    "        indices = [(i, label) for i, (_, label) in enumerate(dataset) if label in classes]\n",
    "        # Remap labels to 0 and 1 for easy classing\n",
    "        label_map = {classes[0]: 0, classes[1]: 1}\n",
    "        indices = [(i, label_map[label]) for i, label in indices]\n",
    "        subset = Subset(dataset, [i for i, _ in indices])\n",
    "        return subset\n",
    "    \n",
    "    #Filtered\n",
    "    train_filtered = filter_ds(train_full)\n",
    "    test_filtered = filter_ds(test_full)\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    train_size = int(len(train_filtered) * train_val_split)\n",
    "    val_size = len(train_filtered) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_filtered, [train_size, val_size])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_filtered\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    " #TODO\n",
    " return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:08<00:00, 2.48MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in CIFAR-10 dataset: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x1af2940a990>,\n",
       " <torch.utils.data.dataset.Subset at 0x1af2940a090>,\n",
       " <torch.utils.data.dataset.Subset at 0x1af2939ebd0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully\n",
    "connected layers) such that:\n",
    "    \n",
    "    - The input dimension is 768(= 16 ∗ 16 ∗ 3) and the output dimension is 2 (for the 2 classes).\n",
    "    - The hidden layers have respectively 128 and 32 hidden units.\n",
    "    - All activation functions are ReLU. The last layer has no activation function since the cross-entropy loss already includes a softmax activation\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2919787615.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    #TODO\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MyNet(nn.Module):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train(n_epochs, optimizer, model, loss_fn, train_loader) function that trains model for n_epochs epochs given an optimizer optimizer, a loss function loss_fn and a dataloader train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    " #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a similar function train manual_update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model using equation (2). Do not forget to zero out all gradients after each iteration. \n",
    "\n",
    "Train 2 instances of MyMLP, one using train and the other using train_manual_update (use the same parameter values for both models). Compare their respective training losses. To get exactly the same results with both functions, see section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
