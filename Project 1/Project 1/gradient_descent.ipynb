{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the same results with train and train_manual_update\n",
    "- Write torch.manual_seed(42) at the beginning of your notebook.\n",
    "- Write torch.set_default_dtype(torch.double) at the beginning of your notebook to alleviate precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "# Set device for the training\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, target_transform=None):\n",
    "        self.subset = subset\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):  # For geetting define length\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3\n",
    "datasets: training, validation and test. Take a subset of these datasets\n",
    "by keeping only 2 labels: cat and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='./data', preprocessor=None):\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(16),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    # Load training dataset\n",
    "    train_dataset = datasets.CIFAR10(data_path, train=True, download=True, transform=preprocessor)\n",
    "    car_cat_indices = [i for i, label in enumerate(train_dataset.targets) if label in (1, 3)]\n",
    "    filtered_subset = Subset(train_dataset, car_cat_indices)\n",
    "    filtered_train = TransformedSubset(filtered_subset, target_transform=lambda y: 0 if y == 1 else 1)\n",
    "    \n",
    "    # Split into training and validation\n",
    "    train_size = int(len(filtered_train) * train_val_split)\n",
    "    val_size = len(filtered_train) - train_size\n",
    "    train_set, val_set = random_split(filtered_train, [train_size, val_size])\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = datasets.CIFAR10(data_path, train=False, download=True, transform=preprocessor)\n",
    "    test_indices = [i for i, label in enumerate(test_dataset.targets) if label in (1, 3)]\n",
    "    test_subset = Subset(test_dataset, test_indices)\n",
    "    test_set = TransformedSubset(test_subset, target_transform=lambda y: 0 if y == 1 else 1)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully\n",
    "connected layers) such that:\n",
    "    \n",
    "    - The input dimension is 768(= 16 ∗ 16 ∗ 3) and the output dimension is 2 (for the 2 classes).\n",
    "    - The hidden layers have respectively 128 and 32 hidden units.\n",
    "    - All activation functions are ReLU. The last layer has no activation function since the cross-entropy loss already includes a softmax activation\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with architecture: 768 -> 128 -> 32 -> 2\n",
    "    Using ReLU activation functions between layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16*16*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train(n_epochs, optimizer, model, loss_fn, train_loader) function that trains model for n_epochs epochs given an optimizer optimizer, a loss function loss_fn and a dataloader train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs: int, optimizer: optim.Optimizer, model: nn.Module, loss_fn, train_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Trains the model for n_epochs using the given optimizer and loss function.\n",
    "    This function only performs training, without validation.\n",
    "    \"\"\"\n",
    "    print(f\"Trener {model.__class__.__name__} med optimizer\")\n",
    "    model.to(device)  # Usin model with device\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())  \n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a similar function train manual_update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model using equation (2). Do not forget to zero out all gradients after each iteration. \n",
    "\n",
    "Train 2 instances of MyMLP, one using train and the other using train_manual_update (use the same parameter values for both models). Compare their respective training losses. To get exactly the same results with both functions, see section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Train the model using manual parameter updates\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    velocities = {name: torch.zeros_like(param.data) for name, param in model.named_parameters() if param.requires_grad}\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    for epoch in range(n_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    grad = param.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        grad = grad + weight_decay * param.data\n",
    "                    velocity = velocities[name]\n",
    "                    velocity = momentum_coeff * velocity + grad\n",
    "                    velocities[name] = velocity\n",
    "                    param.data -= lr * velocity\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de forkjellige resultatene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Batch data shape: torch.Size([64, 3, 16, 16])\n",
      "Batch target shape: torch.Size([64])\n",
      "Trener MyNet med optimizer\n",
      "Training Losses (SGD): [0.6895620485566719, 0.6975296958950579, 0.6925263087280336, 0.6909683872756743, 0.689975278358005]\n",
      "Training Losses (Manual): [0.6895620485566719, 0.6975296958950579, 0.6925263087280336, 0.6909683872756743, 0.689975278358005]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_set, val_set, test_set = load_cifar()\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=False, num_workers=0) # Num_workers = 0 for not in jupyter?\n",
    "    \n",
    "    # Initialize models\n",
    "    torch.manual_seed(42)\n",
    "    model1 = MyNet().to(device)\n",
    "    torch.manual_seed(42)\n",
    "    model2 = MyNet().to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer for model1\n",
    "    optimizer = optim.SGD(model1.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "    \n",
    "    # Debugging: Check DataLoader\n",
    "    for data, target in train_loader:\n",
    "        print(\"Batch data shape:\", data.shape)\n",
    "        print(\"Batch target shape:\", target.shape)\n",
    "        break\n",
    "    \n",
    "    # Train both models\n",
    "    losses1 = train(10, optimizer, model1, loss_fn, train_loader)\n",
    "    losses2 = train_manual_update(10, model2, loss_fn, train_loader, lr=0.01, momentum_coeff=0.9, weight_decay=0.0001)\n",
    "    \n",
    "    # Compare losses\n",
    "    print(\"Training Losses (SGD):\", losses1[:5])\n",
    "    print(\"Training Losses (Manual):\", losses2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing\n",
    "Got Same result for both! Which is what we wanted.\n",
    "Training Losses (SGD) = Training Losses (Manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "Experiment with a wider range of hyperparameters (e.g., learning rates, momentum values, weight decay) to find the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.01, momentum=0.9, weight_decay=0.0001\n",
      "Training accuracy: 0.9012\n",
      "Validation accuracy: 0.8980\n",
      "Training with lr=0.01, momentum=0.9, weight_decay=0.001\n",
      "Training accuracy: 0.9036\n",
      "Validation accuracy: 0.8910\n",
      "Training with lr=0.01, momentum=0.9, weight_decay=0.01\n",
      "Training accuracy: 0.8773\n",
      "Validation accuracy: 0.8790\n",
      "Training with lr=0.01, momentum=0.95, weight_decay=0.0001\n",
      "Training accuracy: 0.9056\n",
      "Validation accuracy: 0.8840\n",
      "Training with lr=0.01, momentum=0.95, weight_decay=0.001\n",
      "Training accuracy: 0.8938\n",
      "Validation accuracy: 0.8760\n",
      "Training with lr=0.01, momentum=0.95, weight_decay=0.01\n",
      "Training accuracy: 0.8576\n",
      "Validation accuracy: 0.8420\n",
      "Training with lr=0.01, momentum=0.99, weight_decay=0.0001\n",
      "Training accuracy: 0.8298\n",
      "Validation accuracy: 0.8160\n",
      "Training with lr=0.01, momentum=0.99, weight_decay=0.001\n",
      "Training accuracy: 0.8179\n",
      "Validation accuracy: 0.8170\n",
      "Training with lr=0.01, momentum=0.99, weight_decay=0.01\n",
      "Training accuracy: 0.7906\n",
      "Validation accuracy: 0.7960\n",
      "Training with lr=0.001, momentum=0.9, weight_decay=0.0001\n",
      "Training accuracy: 0.8109\n",
      "Validation accuracy: 0.8240\n",
      "Training with lr=0.001, momentum=0.9, weight_decay=0.001\n",
      "Training accuracy: 0.8103\n",
      "Validation accuracy: 0.8230\n",
      "Training with lr=0.001, momentum=0.9, weight_decay=0.01\n",
      "Training accuracy: 0.8071\n",
      "Validation accuracy: 0.8210\n",
      "Training with lr=0.001, momentum=0.95, weight_decay=0.0001\n",
      "Training accuracy: 0.8640\n",
      "Validation accuracy: 0.8620\n",
      "Training with lr=0.001, momentum=0.95, weight_decay=0.001\n",
      "Training accuracy: 0.8637\n",
      "Validation accuracy: 0.8600\n",
      "Training with lr=0.001, momentum=0.95, weight_decay=0.01\n",
      "Training accuracy: 0.8511\n",
      "Validation accuracy: 0.8510\n",
      "Training with lr=0.001, momentum=0.99, weight_decay=0.0001\n",
      "Training accuracy: 0.8988\n",
      "Validation accuracy: 0.8900\n",
      "Training with lr=0.001, momentum=0.99, weight_decay=0.001\n",
      "Training accuracy: 0.8982\n",
      "Validation accuracy: 0.8850\n",
      "Training with lr=0.001, momentum=0.99, weight_decay=0.01\n",
      "Training accuracy: 0.8796\n",
      "Validation accuracy: 0.8760\n",
      "Training with lr=0.0001, momentum=0.9, weight_decay=0.0001\n",
      "Training accuracy: 0.6954\n",
      "Validation accuracy: 0.7220\n",
      "Training with lr=0.0001, momentum=0.9, weight_decay=0.001\n",
      "Training accuracy: 0.6956\n",
      "Validation accuracy: 0.7220\n",
      "Training with lr=0.0001, momentum=0.9, weight_decay=0.01\n",
      "Training accuracy: 0.6947\n",
      "Validation accuracy: 0.7220\n",
      "Training with lr=0.0001, momentum=0.95, weight_decay=0.0001\n",
      "Training accuracy: 0.7118\n",
      "Validation accuracy: 0.7540\n",
      "Training with lr=0.0001, momentum=0.95, weight_decay=0.001\n",
      "Training accuracy: 0.7118\n",
      "Validation accuracy: 0.7540\n",
      "Training with lr=0.0001, momentum=0.95, weight_decay=0.01\n",
      "Training accuracy: 0.7103\n",
      "Validation accuracy: 0.7530\n",
      "Training with lr=0.0001, momentum=0.99, weight_decay=0.0001\n",
      "Training accuracy: 0.8017\n",
      "Validation accuracy: 0.8070\n",
      "Training with lr=0.0001, momentum=0.99, weight_decay=0.001\n",
      "Training accuracy: 0.8020\n",
      "Validation accuracy: 0.8070\n",
      "Training with lr=0.0001, momentum=0.99, weight_decay=0.01\n",
      "Training accuracy: 0.7993\n",
      "Validation accuracy: 0.8020\n",
      "\n",
      "Best hyperparameters:\n",
      "Best Learning rate: 0.01\n",
      "Best Momentum: 0.9\n",
      "Best Weight decay: 0.0001\n",
      "Best validation accuracy: 0.8980\n",
      "Training accuracy of the best model: 0.9012\n",
      "Test accuracy of the best model: 0.8800\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "momentum_values = [0.9, 0.95, 0.99]\n",
    "weight_decay_values = [0.0001, 0.001, 0.01]\n",
    "\n",
    "# Initialize variables to track the best model\n",
    "best_accuracy = 0\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "# Create validation and training DataLoader\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=0)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "# Iterate over all combinations of hyperparameters using nested loops\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentum_values:\n",
    "        for weight_decay in weight_decay_values:\n",
    "            print(f\"Training with lr={lr}, momentum={momentum}, weight_decay={weight_decay}\")\n",
    "            \n",
    "            # Initialize model\n",
    "            torch.manual_seed(42)\n",
    "            model = MyNet().to(device)\n",
    "            \n",
    "            # Loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Train model using train_manual_update\n",
    "            train_manual_update(\n",
    "                n_epochs=10,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                train_loader=train_loader,\n",
    "                lr=lr,\n",
    "                momentum_coeff=momentum,\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "            \n",
    "            # Evaluate on training set\n",
    "            train_accuracy = compute_accuracy(model, train_loader)\n",
    "            print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy = compute_accuracy(model, val_loader)\n",
    "            print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Update best model\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_hyperparams = (lr, momentum, weight_decay)\n",
    "                best_model = model\n",
    "\n",
    "# Print best hyperparameters and accuracy\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(f\"Best Learning rate: {best_hyperparams[0]}\")\n",
    "print(f\"Best Momentum: {best_hyperparams[1]}\")\n",
    "print(f\"Best Weight decay: {best_hyperparams[2]}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the training set to check for overfitting\n",
    "train_accuracy = compute_accuracy(best_model, train_loader)\n",
    "print(f\"Training accuracy of the best model: {train_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_accuracy = compute_accuracy(best_model, test_loader)\n",
    "print(f\"Test accuracy of the best model: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
